{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381743fe",
   "metadata": {},
   "source": [
    "## COBW vs BoW\n",
    "\n",
    "In my previous post I show how to predict the next word using a bi-gram model by calculating bi-gram probability\n",
    "\n",
    "https://nbviewer.org/github/kyramichel/NLP/blob/master/N-gram%20Probabilities.ipynb\n",
    "\n",
    " \n",
    "COBW model tries to predict using context ie, it takes into account the similarity between words when predicting the next word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d08bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12363b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents= [\"great beach\", \" awesome beach\", \"great beach and weather\", \"this is a nice beach \",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb9cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'beach', 'awesome', 'beach', 'great', 'beach', 'and', 'weather', 'this', 'is', 'a', 'nice', 'beach']\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "for i in range(len(sents)):\n",
    "    for w in sents[i].split():\n",
    "        tokens.append(w)   \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693bd14",
   "metadata": {},
   "source": [
    "Integer (numeric) representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3eb66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2 3 4 3 1 8 7 5 0 6 3]\n"
     ]
    }
   ],
   "source": [
    "#Integer encoding with LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "integer_encoded = le.fit_transform(tokens)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5cc94",
   "metadata": {},
   "source": [
    "Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511a11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### Binary encoding using OneHotRncoder\n",
    "\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959708c",
   "metadata": {},
   "source": [
    "Limitation of BoW or OneHotEncoding: when trying to predict next word like\"beach\", because using BoW or OneHotEncoding, \"great\" and \"awesome\" are independent - which is not true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2206a",
   "metadata": {},
   "source": [
    "COBW model predicts next word using context, in this case, it takes into account the similarity between \"great\" and \"good\" when trying to predict \"beach\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05559fe5",
   "metadata": {},
   "source": [
    "Word embedding:  capturing context of a word in a document, semantic and syntactic similarity, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3beb76a",
   "metadata": {},
   "source": [
    "Word2Vec algorithm which is implemented in Python, allows to easily construct word embeddings via 2 different methods (both use NN with back-propagation): \n",
    "\n",
    "1. CBOW (Common Bag Of Words): takes the context of each word and predicts a word corresponding to the contex\n",
    "\n",
    "The input (context word) is a OneHotEncoded vector of size V. The hidden layer containing n neurons\n",
    "\n",
    "that takes the weighted average over all the context word inputs. The output is a |V| size vector with the elements being the softmax values.\n",
    "\n",
    "\n",
    "\n",
    "2.Skip Gram\n",
    "\n",
    "Input word (ie context position), the model outputs the probability distributions for each word.\n",
    "\n",
    "Similarity: Words are said to have similar context if they occupy close spatial positions. Mathematically, the cosine of the angle between the vector representations close to 1 when the angle is close to 0. More @ \n",
    "\n",
    "https://arxiv.org/pdf/1310.4546.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1411.2738.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd57814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['great', 'beach'], ['awesome', 'beach'], ['great', 'beach', 'and', 'weather'], ['this', 'is', 'a', 'nice', 'beach']]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    " \n",
    "# iterate through each sentence in the file\n",
    "for s in sents:\n",
    "    temp = []\n",
    "    for w in word_tokenize(s):\n",
    "        temp.append(w.lower())\n",
    "             \n",
    "    data.append(temp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ef9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CBOW, similarity between 'great' and 'awesome' :  0.033640567\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model using Word2Vec algorithm\n",
    "\n",
    "model1= gensim.models.Word2Vec(data, min_count = 1,\n",
    "                              vector_size = 100, window = 5)\n",
    "print(\"Using CBOW, similarity between 'great' \" +\n",
    "               \"and 'awesome' : \",model1.wv.similarity('great', 'awesome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec47917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Skip-Gram method, similarity between 'great' and 'awesome'is :  0.033640567\n"
     ]
    }
   ],
   "source": [
    "# Create Skip Gram model using Word2Vec algorithm\n",
    "\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "\n",
    "print(\"Using Skip-Gram method, similarity between 'great' \" +\n",
    "               \"and 'awesome'is : \", model2.wv.similarity('great', 'awesome'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb4445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
