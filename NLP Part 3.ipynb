{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing (NLP)\n",
    "\n",
    "Analyzing, understanding and generating natural human languages using computers\n",
    "\n",
    "\n",
    "# Web data extraction (Web scraping)\n",
    "\n",
    "\n",
    "\n",
    "### 1. Using Python urllib with urlopen \n",
    "\n",
    "to access data from the web in an automated fashion.\n",
    "\n",
    "\n",
    "Alternatively, you can use a web browser go to specific URL and save the page as text to a\n",
    "local file. Then access text data from computer with open() as shown in NLP Part I\n",
    "\n",
    "\n",
    "\n",
    "Internet contains a large source of text data. Let's access more text from the web ...\n",
    "\n",
    "\n",
    "### Accessing free ebooks from the Project Gutenberg:\n",
    "\n",
    "code below loads that text file at specified URL, just as a browser would: \n",
    "\n",
    "- Instruction print(raw) will displaying it visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading text 2554:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "URL = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "file = urlopen(URL)\n",
    "raw = file.read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "text = word_tokenize(raw)\n",
    "sent = sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'in',\n",
       " 'his',\n",
       " 'writings',\n",
       " '.',\n",
       " 'He',\n",
       " 'describes',\n",
       " 'the',\n",
       " 'awful',\n",
       " 'agony',\n",
       " 'of',\n",
       " 'the',\n",
       " 'condemned',\n",
       " 'man',\n",
       " 'and',\n",
       " 'insists',\n",
       " 'on',\n",
       " 'the',\n",
       " 'cruelty',\n",
       " 'of',\n",
       " 'inflicting',\n",
       " 'such',\n",
       " 'torture']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[600:625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The intense suffering of this experience left a lasting stamp on\\r\\nDostoevsky’s mind.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load text 18440:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "URLbook = \"https://www.gutenberg.org/files/18440/18440-0.txt\"\n",
    "file_book = urlopen(URLbook)\n",
    "raw_book = file_book.read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_book = word_tokenize(raw_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reasoning',\n",
       " ',',\n",
       " \"''\",\n",
       " 'gives',\n",
       " 'an',\n",
       " 'admirably',\n",
       " 'succinct',\n",
       " 'account',\n",
       " 'of',\n",
       " 'their',\n",
       " 'position',\n",
       " '.',\n",
       " 'I',\n",
       " 'agree',\n",
       " 'with']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_book[510:525]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_book = sent_tokenize(raw_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Precise thinking needs precise language                         348\\r\\n§2.',\n",
       " 'Nomenclature and Terminology                                    349\\r\\n§3.',\n",
       " 'Definition                                                      352\\r\\n§4.',\n",
       " 'Rules for testing a Definition                                  352\\r\\n§5.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_book[180:184]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HTML \n",
    "\n",
    "###  urlopen request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title\" content=\"How whales help cool the Earth\"/>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "url =\"https://www.bbc.com/future/article/20210119-why-saving-whales-can-help-fight-climate-change\"\n",
    "\n",
    "file = urlopen(url)\n",
    "html = file.read().decode(\"utf8\")\n",
    "html[799:848]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type print(html) to display the HTML document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python API  requests  \n",
    "\n",
    "\n",
    "Some websites offer data downloadable in text format. \n",
    "\n",
    "\n",
    "Twitter and Facebook provide access to some data through their APIs.\n",
    "\n",
    "\n",
    "- Using Python API (Application Programming Interface) =  server to retrieve (and send) data to using code\n",
    "\n",
    "to access data from the websites that don’t offer such options\n",
    "\n",
    "- Requests = a simple HTTP library for Python  that allows to easily send requests  - no need to manually add query strings to URL\n",
    "\n",
    "\n",
    "The code below makes a get request to a web server that will download the HTML contents of a specified webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get(\"https://www.mcdonalds.com/us/en-us/full-menu.html\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#status code 200 <=> the webpage successfully downloaded\n",
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type print(page.content) #prints the HTML content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Text out of HTML:\n",
    "\n",
    "\n",
    "\n",
    "Using Python's BeautifulSoup to parse HTML \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get text out of HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We’re putting safety first. Learn how.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To find_all <p> tags and extract text:\n",
    "\n",
    "page.find_all('p')[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_page = page.get_text()\n",
    "text_page[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type print(text_page) to display the raw text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nHow whales help cool'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text out of the other HTML file:\n",
    "\n",
    "text_html = BeautifulSoup(html, 'html.parser').get_text()\n",
    "\n",
    "text_html[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nHow whales help cool the Earth - BBC Future\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_html[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using regular Expressions \n",
    "\n",
    "further text cleaning is needed to tokenize the text and remove other elements besides html tags, image maps, JS, forms & tables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('<[^>]*>', '', text) # removes HTML markups \n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) # remove non-word characters and converted the text into lowercase\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) # find emoticons\n",
    "    text = text + \" \".join(emoticons).replace('-', '') #add emoticons at the end \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' how whales help cool the earth bbc future homepageaccessibility linksskip to contentaccessibility h'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_html = clean_text(text_html)\n",
    "clean_text_html[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Text: string to list encoding\n",
    "\n",
    "\n",
    "#### Using regex split:\n",
    "\n",
    "\\s = any whitespace character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'how',\n",
       " 'whales',\n",
       " 'help',\n",
       " 'cool',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'bbc',\n",
       " 'future',\n",
       " 'homepageaccessibility',\n",
       " 'linksskip',\n",
       " 'to',\n",
       " 'contentaccessibility',\n",
       " 'help',\n",
       " 'bbc',\n",
       " 'accountnotifications',\n",
       " 'homenewssportweatheriplayersoundscbbccbeebiesfoodbitesizeartstasterlocalthreemenu',\n",
       " 'searchsearch',\n",
       " 'the',\n",
       " 'bbcsearch']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = re.split(r'\\s+', clean_text_html)\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nltk word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How',\n",
       " 'whales',\n",
       " 'help',\n",
       " 'cool',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '-',\n",
       " 'BBC',\n",
       " 'Future',\n",
       " 'HomepageAccessibility',\n",
       " 'linksSkip',\n",
       " 'to',\n",
       " 'contentAccessibility',\n",
       " 'Help',\n",
       " 'BBC',\n",
       " 'AccountNotifications',\n",
       " 'HomeNewsSportWeatheriPlayerSoundsCBBCCBeebiesFoodBitesizeArtsTasterLocalThreeMenu',\n",
       " 'SearchSearch',\n",
       " 'the',\n",
       " 'BBCSearch']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(text_html)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use useful nltk methods such as nltk findall() we could transform the text from list to a nltk text using nltk.Text(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_html = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open; deep\n"
     ]
    }
   ],
   "source": [
    "#To find all adjectives of ocean\n",
    "\n",
    "Text_html.findall(r\"<the> (<.*>) <ocean>\") #whitespaces between <> are ignored by using nltk findall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Chrome DevTools to Explore HTML\n",
    "\n",
    "\n",
    "https://developers.google.com/web/tools/chrome-devtools\n",
    "\n",
    "Other browsers:\n",
    "\n",
    "- Web Console UI (Firefox)\n",
    "\n",
    "https://developer.mozilla.org/en-US/docs/Tools/Web_Console/UI_Tour\n",
    "\n",
    "- Web Development Tools (Safari)\n",
    "\n",
    "https://developer.apple.com/safari/tools/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
